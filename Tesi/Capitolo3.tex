\chapter{Architettura e Tecnologie Utilizzate}
\label{chap:architettura}

\section{Requisiti del sistema}
\label{sec:requisiti}

Un prototipo di agente conversazionale embodied in Virtual Reality (VR) combina interazione in tempo reale, gestione di asset 3D e servizi di inferenza esterni. Separare requisiti funzionali e non funzionali distingue cosa il sistema deve fare (operazioni osservabili) dai vincoli che rendono l'esperienza stabile e credibile (latenza, disponibilità, modularità).

\subsection{Requisiti funzionali}

Nel prototipo i servizi Speech-to-Text (STT), Retrieval-Augmented Generation (RAG), Text-to-Speech (TTS) e gestione asset avatar sono esposti come micro-servizi FastAPI\footnote{FastAPI Team, \emph{FastAPI Documentation}, \url{https://fastapi.tiangolo.com/}, Accessed: 2026-02-22.} su porte dedicate (8001--8004) e invocati dal client Unity WebGL tramite endpoint HTTP.

RF1 (Libreria e selezione avatar): il client deve ottenere e visualizzare la lista degli avatar disponibili tramite \texttt{/avatars/list} e permettere la selezione di un profilo attivo identificato da \texttt{avatar\_id}. Il requisito è soddisfatto se la lista include almeno un modello locale di fallback (definito in \texttt{LOCAL\_MODELS}) e, quando il backend è raggiungibile, include anche avatar importati con un URL caricabile del modello \texttt{.glb}.

RF2 (Import e caching asset \texttt{.glb}): quando è disponibile un URL di export (ad es. da Avaturn\footnote{Avaturn, \emph{Avaturn Integration Documentation}, \url{https://docs.avaturn.me/docs/integration/overview/}, Accessed: 2026-02-22.}), il client deve richiedere l'import con \texttt{/avatars/import} e ottenere un URL di cache server-side da usare per il download e l'istanza in scena. Il requisito è soddisfatto se import ripetuti dello stesso URL non creano duplicati, mantengono lo stesso \texttt{avatar\_id} con URL di cache stabile, e se l'avatar resta caricabile anche dopo riavvio dei servizi.

RF3 (Setup voce persistente per avatar): il client deve consentire la registrazione di un campione vocale e inviarlo al TTS con \texttt{/set\_avatar\_voice}, associandolo all'avatar attivo; a completamento, il sistema deve poter generare frasi di attesa tramite \texttt{/generate\_wait\_phrases}. Il requisito è soddisfatto se la registrazione supera una verifica di similarità testuale con soglia definita e se il riferimento vocale risulta riutilizzabile nelle sessioni successive; la persistenza è verificabile tramite endpoint di stato voce e/o generazione TTS senza reinvio del campione.

RF4 (Memoria per-avatar con ingest e retrieval): il sistema deve salvare note e contenuti da file nella memoria dell'avatar tramite \texttt{/remember} e \texttt{/ingest\_file}, e deve usare tale memoria nella generazione contestuale via \texttt{/chat}. L'endpoint \texttt{/recall} deve essere disponibile per verifiche e diagnostica del contenuto indicizzato. Il requisito è soddisfatto se la memoria è persistente tra sessioni (finché la directory dati lato server non viene azzerata o cancellata) e se due avatar distinti non condividono documenti indicizzati; l'isolamento può essere verificato confrontando i risultati di \texttt{/recall} a parità di query.

RF5 (Turno vocale end-to-end in push-to-talk): il client deve gestire una conversazione a turni acquisendo audio, inviandolo allo STT con \texttt{/transcribe}, inoltrando la trascrizione al RAG con \texttt{/chat} e riproducendo la risposta audio tramite streaming con \texttt{/tts\_stream}. Il requisito è soddisfatto se, per ogni turno, il sistema produce testo trascritto, testo di risposta e avvio del playback audio con transizioni UI coerenti tra ascolto, elaborazione e riproduzione.

RF6 (Ingest multimodale per memoria per-avatar): il sistema deve consentire l'ingestione multimodale nella memoria dell'avatar, includendo descrizione immagine con \texttt{/describe\_image} e ingestione documentale con \texttt{/ingest\_file} (estrazione e indicizzazione del contenuto). Il requisito è soddisfatto se, dopo l'operazione, i contenuti risultano recuperabili tramite \texttt{/recall} e/o se \texttt{/avatar\_stats} riporta \texttt{has\_memory=true} per l'avatar.

\subsection{Requisiti non funzionali}

I requisiti non funzionali sono formulati secondo il modello ISO/IEC 25010 e si concentrano sulle caratteristiche che influenzano la plausibilità dell'interazione vocale e la sostenibilità tecnica del prototipo. In VR la specifica dei requisiti richiede spesso di dettagliare flussi di scena, artefatti e comportamenti; inoltre cambiamenti minimi possono amplificare costi e complessità, rendendo utile fissare vincoli di qualità verificabili già in fase architetturale \cite{Karre2024VReqST}.

RNF1 (Efficienza prestazionale - time behaviour): la latenza percepita deve restare controllata misurando il tempo tra rilascio del comando push-to-talk e primo campione audio riprodotto dal TTS. Per il prototipo si adotta come soglia prestazionale una risposta udibile entro 5 s in esecuzione locale e entro 8 s in deploy pubblico, tracciando timestamp lato client e tempi di risposta dei servizi; lo streaming riduce l'attesa iniziale. Le soglie 5 s/8 s sono trattate come target empirici per limitare il silenzio percepito e distinguere il comportamento locale da quello in deploy. La conformità al requisito è verificata sui log dei turni vocali (timestamp), calcolando percentuali di turni entro soglia e percentili di latenza nei due contesti (locale/pubblico). Approcci a micro-servizi per agenti sociali real-time trattano la latenza come vincolo primario e riportano tempi dell'ordine di pochi secondi fino alla prima emissione vocale in pipeline ottimizzate \cite{Lin2024Estuary}.

RNF2 (Affidabilità - availability e fault tolerance): durante una sessione di 60 minuti il sistema deve completare con successo tra il 95\% e il 97\% dei turni vocali, dove un turno è riuscito se STT, \texttt{/chat} e \texttt{/tts\_stream} restituiscono esito valido entro timeout. Nel prototipo sono implementati timeout, retry limitati a \texttt{retryCount} (configurabile) e gestione esplicita degli errori lato client; in caso di fallimento il flusso degrada su messaggi di stato UI senza blocchi permanenti. La conformità al requisito è determinata conteggiando turni riusciti e falliti nei log di sessione.

RNF3 (Manutenibilità - modularity e portabilità): la sostituzione di un micro-servizio deve richiedere solo una variazione di configurazione centralizzata lato client (base URL, timeout e policy di retry), mantenendo invariati endpoint e payload attesi. Il requisito si considera soddisfatto quando la stessa build Unity WebGL funziona sia in locale (URL assoluti su loopback) sia in deploy dietro reverse proxy con path \texttt{/api/<servizio>}, e quando dopo un aggiornamento o un redeploy viene rieseguita una checklist minima di compatibilità (\texttt{/health} e una chiamata funzionale per ciascun servizio).

\section{Architettura di riferimento di SOULFRAME}
\label{sec:architettura-riferimento}
\subsection{Vista d'insieme dei componenti frontend/backend}
SOULFRAME adotta un'architettura client--server a componenti, progettata per mantenere sul browser le responsabilità sensibili al frame-rate (interfaccia e resa 3D) e delegare al backend i compiti di inferenza e persistenza. Il client è una build Unity WebGL eseguita nel browser: gestisce la UI, le transizioni di stato dell'esperienza e l'interazione push-to-talk, oltre al rendering e alla visualizzazione dell'avatar. La comunicazione verso il backend avviene via HTTP e viene normalizzata tramite una configurazione centralizzata che astrae la differenza tra esecuzione locale (URL e porte esplicite su loopback) e deploy pubblico (path relativi sotto un unico origin).

Sul backend, la pipeline è scomposta in quattro micro-servizi FastAPI indipendenti, ciascuno con un contratto API mirato. Whisper (porta 8001) espone \texttt{/transcribe} e trasforma l'audio in testo. Il servizio RAG (porta 8002) governa l'orchestrazione conversazionale con \texttt{/chat}, insieme alla memoria per-avatar (\texttt{/remember}, \texttt{/recall}) e all'ingestione di contenuti (\texttt{/ingest\_file}). In questa architettura il RAG è il punto di convergenza: costruisce il contesto via retrieval e delega a un runtime esterno le operazioni di Large Language Model (LLM) ed embedding. Il servizio Text-to-Speech (TTS), basato su Coqui XTTS v2 \cite{Casanova2024XTTS} e in ascolto sulla porta 8004, espone sintesi completa (\texttt{/tts}), sintesi in streaming (\texttt{/tts\_stream}) e endpoint per frasi di attesa (\texttt{/wait\_phrase}, \texttt{/generate\_wait\_phrases}). L'Avatar Asset Server (porta 8003) gestisce lista e import degli avatar (\texttt{/avatars/list}, \texttt{/avatars/import}) e serve i modelli \texttt{.glb} tramite \texttt{/avatars/{id}/model.glb}, mantenendo persistente la cache degli asset tra sessioni.

Il servizio di supporto Ollama opera come runtime separato per LLM ed embeddings sulla porta 11434. Nel prototipo il profilo del modello di chat può variare tra ambiente locale e server con GPU dedicata (profilo lightweight vs profilo standard), mentre il modello embedding resta dedicato al retrieval. L'uso di Ollama come runtime separato mantiene il micro-servizio RAG focalizzato sull'orchestrazione applicativa (memoria, retrieval e composizione della richiesta) e consente di sostituire o aggiornare i modelli senza modificare il client, a patto di preservare endpoint e formati attesi. I dettagli di confronto tra configurazioni modello sono discussi nel Capitolo 4.

In deploy pubblico, l'elemento chiave di integrazione è il reverse proxy Caddy, che svolge due funzioni: terminazione TLS e punto d'ingresso unico per il browser. Il client WebGL comunica con un solo origin HTTPS e invoca le API tramite path riscritti (\texttt{/api/whisper/*}, \texttt{/api/rag/*}, \texttt{/api/avatar/*}, \texttt{/api/tts/*}); Caddy inoltra le richieste alle porte interne 8001--8004, mantenute non esposte verso l'esterno. Il reverse proxy riduce la complessità lato browser (stessa origine, routing consistente) e isola i servizi, che restano indirizzabili e gestibili separatamente a livello di processo. Nel prototipo, gli endpoint non implementano autenticazione/autorizzazione applicativa e l'architettura assume una rete di esecuzione controllata. Figura~\ref{fig:arch-cap3} riassume questa vista a blocchi, chiarendo sia la separazione di responsabilità tra frontend e backend sia il ruolo del proxy come snodo di comunicazione e sicurezza.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{arch_cap3.png}
\caption{Vista d'insieme dell'architettura a componenti di SOULFRAME: il client Unity WebGL comunica con i micro-servizi backend attraverso il reverse proxy Caddy.}
\label{fig:arch-cap3}
\end{figure}

Dal punto di vista operativo, la stessa scomposizione in componenti viene mantenuta sia in locale sia su server. In ambiente Windows lo script di avvio coordina l'esecuzione dei servizi e applica controlli di base (ad esempio disponibilità delle porte) per ridurre conflitti tra processi durante lo sviluppo. In ambiente Ubuntu, ciascun micro-servizio è gestito come unità \texttt{systemd}, con comandi amministrativi che permettono start/stop/status e aggiornamenti senza dover riconfigurare manualmente l'intero stack. Nel lavoro di sviluppo ho mantenuto questa separazione perché i colli di bottiglia più difficili da diagnosticare emergevano tra servizi diversi, non dentro un singolo modulo.

\subsection{\texorpdfstring{Flusso end-to-end audio $\rightarrow$ testo $\rightarrow$ risposta $\rightarrow$ audio}{Flusso end-to-end audio -> testo -> risposta -> audio}}
Il flusso conversazionale end-to-end di SOULFRAME è organizzato come una pipeline a turni che parte dall'input vocale dell'utente e termina con la riproduzione della risposta sintetizzata, mantenendo sul client Unity WebGL le responsabilità di interazione e rendering e delegando al backend le fasi di inferenza. L'interazione è di tipo push-to-talk: l'utente tiene premuto \texttt{SPACE} per parlare e, al rilascio, il client chiude la registrazione e prepara un file audio in formato WAV, includendo un parametro di lingua nella richiesta. In ambiente WebGL l'acquisizione del microfono e la gestione del contesto audio del browser richiedono un bridge JavaScript, qui realizzato tramite un plugin (\texttt{AudioCapture.jslib}) richiamato dal componente di registrazione in Unity, così da appoggiarsi alle primitive audio disponibili nel runtime WebGL.\footnote{Unity Technologies, \emph{Audio in WebGL}, \url{https://docs.unity3d.com/6000.2/Documentation/Manual/webgl-audio.html}, Accessed: 2026-02-22.}

Una volta completata la cattura, il client invia l'audio al servizio Speech-to-Text (STT) basato su Whisper tramite una richiesta \texttt{POST /transcribe}. Il micro-servizio esegue la trascrizione e restituisce un JSON che contiene il testo riconosciuto nel campo \texttt{"text"}. L'isolamento dell'Automatic Speech Recognition (ASR) in un componente dedicato rende esplicita la prima trasformazione del flusso, da segnale audio (WAV) a contenuto testuale (stringa), su cui è poi possibile applicare logiche conversazionali e di memoria.

La trascrizione viene quindi inoltrata al servizio RAG/LLM con una richiesta \texttt{POST /chat} che include l'identificativo dell'avatar (\texttt{avatar\_id}). In questo stadio il backend svolge la funzione di orchestratore: recupera contesto dalla memoria per-avatar, costruita su un database vettoriale persistente (ChromaDB\footnote{Chroma, \emph{Chroma Documentation}, \url{https://docs.trychroma.com/}, Accessed: 2026-02-22.}) e popolata tramite embedding generati con un modello dedicato (nella configurazione corrente \texttt{nomic-embed-text} erogato da Ollama). Il retrieval, quando disponibile memoria, produce un insieme di passaggi contestuali che vengono integrati nel prompt e usati per vincolare e arricchire la generazione. Il servizio invoca poi il modello linguistico per la risposta (nella configurazione corrente \texttt{llama3:8b-instruct-q4\_K\_M} via Ollama), ottenendo un testo finale che viene restituito al client come contenuto della risposta. In questa fase si concentrano sia la dipendenza dalla memoria dell'avatar sia la variabilità computazionale dovuta alla generazione, motivo per cui la gestione della latenza percepita diventa parte integrante del disegno architetturale.

Ricevuto il testo di risposta, il client attiva la sintesi vocale tramite il servizio Text-to-Speech (TTS) basato su Coqui XTTS v2. La richiesta avviene preferibilmente tramite l'endpoint di streaming \texttt{/tts\_stream}, includendo \texttt{avatar\_id} e lingua: il servizio recupera il profilo vocale associato all'avatar (voice cloning) e produce audio progressivamente, consentendo al client di iniziare il playback non appena arrivano i primi byte dello stream. Lo streaming riduce il tempo tra fine turno utente e inizio della risposta udibile, perché sposta l'attesa dal completamento dell'intero file audio alla sola generazione dell'incipit, migliorando la continuità percepita anche quando la risposta è lunga.

SOULFRAME introduce inoltre strategie specifiche per mitigare i tempi morti più evidenti. Per contenere la latenza iniziale, il servizio TTS esegue un warmup durante il boot: all'avvio genera una frase breve (ad esempio ``Ciao.'') per inizializzare modello e dipendenze, riducendo il costo della prima richiesta reale; in parallelo, il frontend mostra un pannello di caricamento dedicato e rende disponibili le modalità operative solo quando il TTS risulta pronto. Lo stesso problema può riemergere durante i turni successivi, quando la pipeline STT$\rightarrow$RAG/LLM$\rightarrow$TTS è in corso: in questa fase il sistema può riprodurre brevi vocalizzi o frasi di attesa pre-generate (ad esempio ``hm'' o ``un secondo'') per evitare silenzi prolungati e dare un segnale immediato di reattività. Le wait phrases sono associate al profilo vocale dell'avatar e vengono generate una tantum o rigenerate quando necessario; dal punto di vista architetturale costituiscono un canale asincrono di feedback che non altera il contenuto della risposta, ma agisce sulla percezione temporale dell'utente.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{ui_state_diagram_cap3.png}
\caption{Diagramma degli stati UI del client SOULFRAME: dalla fase
di caricamento iniziale al ciclo conversazionale
(Idle $\rightarrow$ Listening $\rightarrow$ Processing $\rightarrow$ Speaking)
e alla gestione degli errori.}
\label{fig:ui-state-diagram-cap3}
\end{figure}

Figura~\ref{fig:flusso-e2e-cap3} sintetizza la sequenza temporale e i formati dati scambiati tra componenti, mettendo in evidenza dove avvengono le trasformazioni principali (WAV$\rightarrow$testo, testo$\rightarrow$testo contestualizzato, testo$\rightarrow$audio) e dove si innestano le tecniche di riduzione della latenza percepita (warmup, wait phrases, streaming). La stessa vista è utile anche per chiarire che, in deploy con reverse proxy, gli endpoint REST restano invariati a livello logico e vengono solo raggiunti tramite path riscritti sotto un unico origin HTTPS.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{flusso_e2e_cap3.png}
\caption{Flusso end-to-end di una richiesta conversazionale in SOULFRAME: dall'acquisizione audio push-to-talk alla riproduzione della risposta vocale.}
\label{fig:flusso-e2e-cap3}
\end{figure}

\subsection{Flusso di gestione avatar (creazione, import, cache, rendering)}
Il flusso di gestione degli avatar in SOULFRAME affianca due percorsi distinti, che convergono poi sulla stessa esperienza di utilizzo in scena: da un lato gli avatar locali pre-inclusi nella build, dall'altro gli avatar importati, creati dall'utente tramite Avaturn. L'obiettivo architetturale è garantire che il client Unity WebGL possa sempre offrire una libreria minima di modelli selezionabili, mantenendo allo stesso tempo un canale controllato per acquisire, validare e servire asset esterni in formato \texttt{.glb} senza dipendere direttamente da URL remoti durante il rendering.

Gli avatar locali costituiscono il percorso più semplice: i modelli \texttt{LOCAL\_model1} e \texttt{LOCAL\_model2} sono sempre presenti e vengono esposti nella lista restituita dal backend insieme agli avatar importati. A livello di interazione, l'utente naviga la libreria tramite l'interfaccia a carosello e seleziona un profilo; il client aggiorna quindi lo stato dell'avatar corrente e avvia il caricamento del modello nella scena. In questa modalità non è necessario alcun passaggio di import o caching, poiché l'asset è già disponibile nel pacchetto o in una posizione nota al runtime Unity, e il flusso si riduce a selezione e rendering.

Il percorso degli avatar importati introduce invece un passaggio di creazione esterna, gestito come overlay nel browser. Dal menu del client, l'utente avvia la creazione aprendo l'esperienza Avaturn in un iframe: \texttt{AvaturnWebController.cs} richiede l'apertura dell'overlay e delega la gestione dell'iframe al bridge JavaScript, che incapsula il ciclo di vita dell'interazione (apertura, caricamento, chiusura) e la ricezione dell'evento di export. Quando l'utente termina la personalizzazione, Avaturn produce un export del modello \texttt{.glb} e il bridge (\texttt{AvaturnBridge.jslib}) inoltra al runtime Unity un payload JSON con URL, \texttt{avatarId} e metadati (ad esempio \texttt{gender}, \texttt{bodyId}, \texttt{urlType}) sfruttando il meccanismo standard di interoperabilità Unity WebGL tra JavaScript e C\# basato su plugin \texttt{.jslib} e chiamate di messaggistica verso GameObject.\footnote{Unity Technologies, \emph{Interaction with browser scripting in WebGL}, \url{https://docs.unity3d.com/6000.2/Documentation/Manual/webgl-interacting-browser-js.html}, Accessed: 2026-02-22.} Il client riceve il JSON e lo delega al gestore avatar, che avvia la fase di import.

L'import nel backend avviene tramite l'endpoint \texttt{/avatars/import} del servizio \texttt{avatar\_asset\_server.py}. La richiesta include \texttt{avatar\_id}, URL del file \texttt{.glb} esportato e metadati utili a ricostruire il profilo dell'avatar. Il server scarica il file e lo memorizza in una directory di storage dedicata (\texttt{avatar\_store/}), aggiornando un file di metadati JSON che mantiene la lista degli avatar importati. La cache server-side che ne risulta libera il client dalla dipendenza dall'URL originario di Avaturn durante le sessioni successive: il modello viene caricato da un endpoint controllato e coerente con il deploy. Il modello viene poi esposto tramite \texttt{GET /avatars/\{id\}/model.glb} e pubblicato al client come \texttt{cached\_glb\_url} all'interno della risposta di \texttt{/avatars/list}, che aggrega sempre avatar locali e importati in un'unica libreria.

Un aspetto rilevante di robustezza è la logica di self-healing dei metadati implementata dal servizio asset: in fase di listing, il server risolve \texttt{file\_path} verificando l'esistenza del file e, se necessario, ricostruisce il collegamento individuando il modello corrispondente nella cache (ad esempio tramite pattern sul nome e timestamp). In presenza di deploy o migrazioni che cambiano percorsi assoluti o struttura delle directory, questa strategia riduce la probabilità di riferimenti obsoleti e mantiene la lista coerente con lo stato effettivo dello storage.

Dal lato client, una volta ottenuto \texttt{cached\_glb\_url}, il runtime Unity scarica il modello e lo istanzia nello spawn point dell'avatar, aggiornando i riferimenti di scena e lo stato dell'avatar corrente. Il caricamento e lo switching vengono orchestrati dal gestore avatar e dall'interfaccia della libreria, così che la transizione tra profili sia percepita come un'operazione di selezione indipendente dalla provenienza (locale o importata). Figura~\ref{fig:flusso-avatar-cap3} riassume i due percorsi e il punto di convergenza, chiarendo dove intervengono iframe Avaturn, bridge WebGL, caching server-side e download del \texttt{.glb} prima del rendering in Unity.

Dopo la selezione o creazione, il flusso passa al setup voce dell'avatar. In questa fase la soglia del 70\% non rappresenta una verifica biometrica: il client registra un campione, lo invia a \texttt{/transcribe} e confronta la trascrizione con la frase attesa mostrata in UI, applicando una metrica di similarità testuale. Il campione viene accettato solo se lo score supera la soglia \texttt{0.7}; in caso contrario l'utente viene invitato a ripetere la registrazione. Superata la verifica, il client persiste il profilo vocale tramite \texttt{/set\_avatar\_voice} e avvia la generazione delle frasi di attesa con \texttt{/generate\_wait\_phrases}.

Completato il setup voce, l'onboarding verifica la memoria per-avatar tramite \texttt{/avatar\_stats?avatar\_id=...} (campo \texttt{has\_memory}). Se la memoria è assente, il client indirizza a \texttt{SetupMemory}, che offre tre modalità: nota manuale (\texttt{/remember}, con metadati di provenienza, ad es. \texttt{source\_type=manual}), ingestione file \texttt{.pdf}/\texttt{.txt} via \texttt{/ingest\_file} (estrazione del contenuto e indicizzazione nella memoria vettoriale), e descrizione immagine via \texttt{/describe\_image} con salvataggio della descrizione in memoria. Questo passaggio popola dati testuali e metadati associati all'\texttt{avatar\_id}, rendendo il retrieval disponibile già dai primi turni di \texttt{/chat}; la presenza dei contenuti è verificabile sia con \texttt{/recall} sia con \texttt{/avatar\_stats}. I dettagli implementativi della pipeline di estrazione e indicizzazione sono discussi nel Capitolo~\ref{chap:implementazione}. In termini di stato applicativo, il client può applicare un redirect automatico al setup mancante prima dell'accesso a \texttt{MainMode}. Completato setup avatar (voce/memoria), l'avatar diventa soggetto attivo della pipeline conversazionale descritta in 3.2.2. Alcune schermate operative del client sono riportate in Figura~\ref{fig:ui-carosello-cap3}, Figura~\ref{fig:ui-ptt-cap3} e Figura~\ref{fig:ui-mainmode-cap3}.

\begin{figure}[t]
\centering
\includegraphics[width=0.88\textwidth]{flusso_avatar_cap3.png}
\caption{Flusso di gestione avatar in SOULFRAME: dalla creazione tramite Avaturn alla cache server-side e al rendering nel client Unity.}
\label{fig:flusso-avatar-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_carosello.png}
\caption{Interfaccia carosello avatar in SOULFRAME: selezione del
profilo attivo tra avatar locali e importati.}
\label{fig:ui-carosello-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_push_to_talk.png}
\caption{Stato di ascolto push-to-talk nel client Unity WebGL:
acquisizione audio attiva con indicatore visivo.}
\label{fig:ui-ptt-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_mainmode.png}
\caption{MainMode conversazionale in SOULFRAME: trascrizione e
risposta dell'avatar in corso durante un turno vocale.}
\label{fig:ui-mainmode-cap3}
\end{figure}

\section{Componenti implementati}
Questa sezione descrive i principali componenti effettivamente realizzati nel prototipo SOULFRAME, mettendo in relazione le scelte implementative con l'architettura logica presentata nella Sezione~\ref{sec:architettura-riferimento}. L'obiettivo è chiarire quali moduli concretizzano i flussi discussi nel capitolo, senza entrare nei dettagli più minuti di singole funzioni o ottimizzazioni, che verranno affrontati nel Capitolo~4. In particolare, si analizzano l'integrazione di Avaturn nel client Unity WebGL, l'implementazione dei micro-servizi AI e la gestione della persistenza per profilo avatar; la panoramica unificata dei micro-servizi backend è riportata in Tabella~\ref{tab:microservizi-cap3}.

\subsection{Integrazione Avaturn nel frontend Unity WebGL}
L'integrazione di Avaturn nel frontend è realizzata incapsulando l'editor di personalizzazione in un overlay iframe aperto dal client Unity WebGL. La gestione del ciclo di vita dell'overlay (apertura, ricezione evento export, chiusura) è delegata a un bridge JavaScript che inoltra al runtime Unity un payload JSON con URL del modello \texttt{.glb} e metadati dell'avatar tramite il meccanismo standard di messaggistica \texttt{.jslib}.\footnote{Unity Technologies, \emph{Interaction with browser scripting in WebGL}, \url{https://docs.unity3d.com/6000.2/Documentation/Manual/webgl-interacting-browser-js.html}, Accessed: 2026-02-22.} Il client riceve il JSON e delega la fase di import al backend; da quel momento il flusso converge sul percorso descritto in Sezione~\ref{sec:architettura-riferimento}. I dettagli implementativi del bridge (gestione del DOM, interoperabilità C\#/JavaScript, compatibilità con il prefab Avaturn) sono discussi nel Capitolo~\ref{chap:implementazione}.

\subsection{Backend AI a micro-servizi (Whisper, RAG, TTS, Avatar Asset)}
Il backend di SOULFRAME è realizzato come insieme di micro-servizi indipendenti basati su FastAPI, avviati con \texttt{uvicorn} su porte dedicate (8001--8004). La scomposizione in servizi rende esplicita la separazione delle responsabilità e consente di verificare disponibilità e corretto instradamento con controlli semplici (\texttt{/health}) prima di abilitare il flusso conversazionale. Tabella~\ref{tab:microservizi-cap3} sintetizza i quattro componenti e i relativi contratti API: il riepilogo è utile sia in fase di sviluppo (per individuare rapidamente dipendenze e punti di failure) sia per la lettura del capitolo, perché connette porte, endpoint e responsabilità operative in un'unica vista.

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\raggedright\arraybackslash}p{0.16\textwidth}|>{\raggedright\arraybackslash}p{0.20\textwidth}|>{\raggedright\arraybackslash}p{0.07\textwidth}|>{\raggedright\arraybackslash}p{0.24\textwidth}|>{\raggedright\arraybackslash}p{0.27\textwidth}|}
\hline
\textbf{Componente} & \textbf{File Python} & \textbf{Porta} & \textbf{Endpoint chiave} & \textbf{Responsabilità} \\
\hline
Whisper STT & \path{whisper_server.py} & \texttt{8001} & \path{GET /health}\ \path{POST /transcribe} & Trascrizione Speech-to-Text (STT) di audio caricato (WAV o formati compatibili), gestione lingua e cleanup di file temporanei. \\
\hline
RAG/LLM & \path{rag_server.py} & \texttt{8002} & \path{GET /health}\ \path{GET /avatar_stats}\ \path{POST /chat}\ \path{POST /remember}\ \path{POST /recall}\ \path{POST /ingest_file}\ \path{POST /describe_image} & Orchestrazione Retrieval-Augmented Generation (RAG): memoria per-avatar su ChromaDB, retrieval ibrido (similarità vettoriale + BM25), chiamate a Ollama per LLM/embedding e ingestione multimodale con estrazione/indicizzazione dei contenuti. \\
\hline
Coqui XTTS TTS & \path{coqui_tts_server.py} & \texttt{8004} & \path{GET /health}\ \path{POST /tts}\ \path{POST /tts_json}\ \path{POST /set_avatar_voice}\ \path{GET/DELETE /avatar_voice}\ \path{POST /generate_wait_phrases}\ \path{GET /wait_phrase} & Sintesi Text-to-Speech (TTS) con voice cloning per avatar, streaming audio e generazione/serving di frasi di attesa; gestione e persistenza dei profili vocali per \texttt{avatar\_id}. \\
\hline
Avatar Asset Server & \path{avatar_asset_server.py} & \texttt{8003} & \path{GET /health}\ \path{GET /avatars/list}\ \path{POST /avatars/import}\ \path{GET /avatars/{id}/model.glb}\ \path{DELETE /avatars/{id}} & Import e caching server-side di modelli \texttt{.glb}, deduplicazione via hash URL, self-healing dei metadati e lista unificata di avatar locali di fallback e avatar importati. \\
\hline
\end{tabular}
\caption{Riepilogo dei micro-servizi backend di SOULFRAME: porte, endpoint principali e responsabilità operative.}
\label{tab:microservizi-cap3}
\end{table}

Whisper presidia la trasformazione audio$\rightarrow$testo e costituisce il punto di ingresso del canale vocale utente. Il servizio RAG/LLM governa memoria per-avatar e generazione contestuale, coordinando retrieval e risposta conversazionale. Il servizio TTS converte la risposta in audio, riusando profili vocali per-avatar e gestendo le frasi di attesa. L'Avatar Asset Server isola il ciclo di vita dei modelli \texttt{.glb} (import, cache e distribuzione) dal resto della pipeline conversazionale. In questa sottosezione, il termine \emph{ricerca ibrida} indica la combinazione di similarità vettoriale e BM25 nella fase di retrieval del RAG.

La scomposizione in servizi autonomi mantiene il client Unity disaccoppiato dalla logica interna di inferenza e storage, rendendo più semplice evolvere singoli componenti senza cambiare il flusso utente. In pratica ho mantenuto questo vincolo per poter aggiornare STT, RAG o TTS in modo indipendente durante i test comparativi. I dettagli implementativi (caricamento modelli, gestione file temporanei, chunking/overlap, estrazione contenuto, deduplicazione, self-healing e parametri runtime) sono discussi nel Capitolo~\ref{chap:implementazione}.
La documentazione automatica degli endpoint è riportata in Figura~\ref{fig:rag-swagger-cap3}.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{rag_docs_swagger.png}
\caption{Interfaccia Swagger UI (\texttt{/docs}) del servizio RAG di
SOULFRAME: gli endpoint esposti da \texttt{rag\_server.py} con
schema automatico generato da FastAPI.}
\label{fig:rag-swagger-cap3}
\end{figure}

\subsection{Persistenza e gestione dati per avatar}

La persistenza dei dati in SOULFRAME è organizzata su filesystem e segue una separazione per \texttt{avatar\_id}: ogni profilo mantiene in modo indipendente tre store dedicati, ovvero \texttt{avatar\_store/} per gli asset \texttt{.glb}, \texttt{voices/avatars/} per il profilo vocale e \texttt{rag\_store/} per la memoria conversazionale. Questa struttura mantiene allineati identità dell'avatar e risorse operative lungo tutto il ciclo di vita del profilo.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{persistenza_tree_cap3.png}
\caption{Struttura del filesystem di persistenza in SOULFRAME:
tre store isolati per \texttt{avatar\_id} (asset \texttt{.glb},
profilo vocale e memoria RAG).}
\label{fig:persistenza-tree-cap3}
\end{figure}

L'isolamento per profilo consente di evitare contaminazioni tra avatar distinti e di ricostruire lo stato dopo riavvio rileggendo i rispettivi percorsi persistenti. A livello applicativo, la presenza di memoria può essere verificata tramite \texttt{/avatar\_stats} (campo \texttt{has\_memory}), mentre asset e voce restano associati allo stesso \texttt{avatar\_id} nei rispettivi store.

I dettagli implementativi (deduplicazione, scrittura atomica, self-healing dei metadati, generazione lazy delle frasi di attesa) sono discussi nel Capitolo~\ref{chap:implementazione}.

\section{Setup e deploy operativo}
Il setup e il deploy operativo rendono concreta l'architettura descritta nella Sezione~\ref{sec:architettura-riferimento}, traducendo la scomposizione in componenti in procedure ripetibili di avvio, arresto e verifica. SOULFRAME mantiene la stessa struttura logica in locale e in produzione, ma cambia l'orchestrazione dei processi e il punto di ingresso di rete, in coerenza con i vincoli di portabilità e manutenibilità discussi in Sezione~\ref{sec:requisiti}. Per ridurre errori di esercizio, ho privilegiato passaggi operativi ripetibili e un punto di controllo unificato per i servizi.

\subsection{Ambiente locale Windows}
La configurazione locale è orientata a sviluppo e test rapidi: i servizi girano su loopback e la build Unity WebGL è servita in HTTP. Il provisioning iniziale è automatizzato da \texttt{setup\_soulframe\_windows.bat}, che prepara ambiente Python, dipendenze backend e configurazioni base necessarie all'avvio del prototipo. In questa modalità l'obiettivo non è la hardening dell'infrastruttura, ma la riduzione del tempo tra modifica del codice e verifica end-to-end.

L'avvio operativo è centralizzato in \texttt{ai\_services.cmd}, che coordina i processi applicativi, riduce i conflitti su porte già occupate e applica una sequenza di start coerente con le dipendenze della pipeline. In pratica, lo sviluppatore mantiene un punto unico per start/stop/restart dello stack locale, evitando avvii manuali separati dei micro-servizi.

Sul piano funzionale, questo assetto favorisce debugging iterativo e osservabilità immediata: i log dei servizi sono disponibili in console, gli endpoint risultano interrogabili direttamente su loopback e il client WebGL può essere validato senza introdurre variabili infrastrutturali esterne. Le differenze operative rispetto al deploy server sono sintetizzate in Tabella~\ref{tab:deploy-confronto-cap3}.

\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\raggedright\arraybackslash}p{0.19\textwidth}|>{\raggedright\arraybackslash}p{0.35\textwidth}|>{\raggedright\arraybackslash}p{0.35\textwidth}|}
\hline
\textbf{Aspetto} & \textbf{Windows locale} & \textbf{Ubuntu server} \\
\hline
Avvio servizi & Script locale (\texttt{ai\_services.cmd}) con processi separati in ascolto su loopback & Service manager con unità dedicate e restart automatico \\
\hline
Origin/URL usati dal client & Origin locale (\texttt{http://localhost:8000}) con porte backend esplicite in configurazione client & Origin HTTPS unico; API raggiunte via path \texttt{/api/<servizio>} dietro reverse proxy \\
\hline
TLS & Assente nel loop di sviluppo locale (HTTP) & Terminazione TLS al reverse proxy \\
\hline
Logging & Log di console/processo per debugging interattivo & Log centralizzati di servizi e reverse proxy \\
\hline
Update/rollback & Aggiornamento manuale dello workspace e riavvio script & Script amministrativi con stop/start orchestrato e supporto backup/rollback \\
\hline
Gestione persistenze & Directory locali del progetto (\texttt{avatar\_store}, \texttt{voices}, \texttt{rag\_store}) & Directory persistenti lato server con separazione per componente/avatar \\
\hline
\end{tabular}
\caption{Confronto operativo tra setup locale Windows e deploy server Ubuntu.}
\label{tab:deploy-confronto-cap3}
\end{table}

\subsection{Ambiente server Ubuntu}
In ambiente Ubuntu il deploy è orientato all'esposizione pubblica controllata del prototipo. I micro-servizi restano su rete interna, mentre il browser raggiunge un unico origin HTTPS tramite reverse proxy. L'adozione di un origin HTTPS unico semplifica l'integrazione lato client (stessa origine, path applicativi uniformi) e riduce la superficie di esposizione diretta delle singole porte interne.

Il setup server organizza artefatti e configurazione runtime in percorsi dedicati, separando componenti applicativi, file statici WebGL e variabili ambientali operative. La gestione dei processi è demandata a unità del service manager con politiche di restart, così che il ripristino dopo reboot o failure non richieda intervento manuale su ogni micro-servizio.

Dal punto di vista del routing, il reverse proxy gestisce terminazione TLS, serving dei file statici e inoltro delle richieste API verso i servizi interni. In questo modo la stessa build Unity WebGL resta riutilizzabile tra locale e server, variando principalmente il livello di orchestrazione e il punto di ingresso di rete. Anche in questo caso i dettagli puntuali di provisioning e parametrizzazione sono rinviati al Capitolo~\ref{chap:implementazione}.

\subsection{Servizi di supporto (systemd, Caddy, script amministrativi)}
Accanto ai micro-servizi applicativi, il prototipo include un livello di supporto operativo che copre tre esigenze: lifecycle dei processi, gateway/reverse proxy e automazione amministrativa. Questo livello è cruciale per mantenere ripetibile la gestione dell'ambiente, soprattutto quando la piattaforma viene aggiornata o riavviata frequentemente.

Il service manager fornisce avvio al boot, restart automatico e consultazione unificata dei log, consentendo interventi mirati su singoli servizi o sull'intero stack. Il reverse proxy concentra invece le responsabilità di ingress (TLS, routing API e serving statico), così da mantenere il browser disaccoppiato dai dettagli di rete interni.

Gli script amministrativi completano il quadro con procedure standardizzate di manutenzione (stato, restart, aggiornamento, backup e rollback), riducendo il rischio di azioni manuali non ripetibili. La tabella comparativa precedente rende verificabili queste differenze tra ambienti senza appesantire la lettura del capitolo; i dettagli esecutivi sono approfonditi nel Capitolo~\ref{chap:implementazione}.
