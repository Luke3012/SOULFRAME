\chapter{Architettura e Tecnologie Utilizzate}
\label{chap:architettura}

\section{Requisiti del sistema}
\label{sec:requisiti}

Un prototipo di agente conversazionale embodied in Virtual Reality (VR) combina interazione in tempo reale, gestione di asset 3D e servizi di inferenza esterni. Separare requisiti funzionali e non funzionali distingue cosa il sistema deve fare (operazioni osservabili) dai vincoli che rendono l'esperienza stabile e credibile (latenza, disponibilità, modularità).

\subsection{Requisiti funzionali}

In SOULFRAME i servizi Speech-to-Text (STT), Retrieval-Augmented Generation (RAG), Text-to-Speech (TTS) e gestione asset avatar sono esposti come micro-servizi FastAPI\footnote{FastAPI Team, \emph{FastAPI Documentation}, \url{https://fastapi.tiangolo.com/}} su porte dedicate (8001--8004) e invocati dal client Unity WebGL tramite endpoint HTTP.

RF1 (Libreria e selezione avatar): il client deve ottenere e visualizzare la lista degli avatar disponibili tramite \texttt{/avatars/list} e permettere la selezione di un profilo attivo identificato da \texttt{avatar\_id}. Il requisito è soddisfatto se la lista include almeno un modello locale di fallback (definito in \texttt{LOCAL\_MODELS}) e, quando il backend è raggiungibile, include anche avatar importati con un URL caricabile del modello \texttt{.glb}.

RF2 (Import e caching asset \texttt{.glb}): quando è disponibile un URL di export (ad es. da Avaturn\footnote{Avaturn, \emph{Avaturn Integration Documentation}, \url{https://docs.avaturn.me/docs/integration/overview/}}), il client deve richiedere l'import con \texttt{/avatars/import} e ottenere un URL di cache server-side da usare per il download e l'istanza in scena. Il requisito è soddisfatto se import ripetuti dello stesso URL non creano duplicati, mantengono lo stesso \texttt{avatar\_id} con URL di cache stabile, e se l'avatar resta caricabile anche dopo riavvio dei servizi.

RF3 (Setup voce persistente per avatar): il client deve consentire la registrazione di un campione vocale e inviarlo al TTS con \texttt{/set\_avatar\_voice}, associandolo all'avatar attivo; a completamento, il sistema deve poter generare frasi di attesa tramite \texttt{/generate\_wait\_phrases}. Il requisito è soddisfatto se la registrazione supera una verifica di similarità testuale con soglia definita e se il riferimento vocale risulta riutilizzabile nelle sessioni successive; la persistenza è verificabile tramite endpoint di stato voce e/o generazione TTS senza reinvio del campione.

RF4 (Memoria per-avatar con ingest e retrieval): il sistema deve salvare note e contenuti da file nella memoria dell'avatar tramite \texttt{/remember} e \texttt{/ingest\_file}, e deve usare tale memoria nella generazione contestuale via \texttt{/chat}. L'endpoint \texttt{/recall} deve essere disponibile per verifiche e diagnostica del contenuto indicizzato. Il requisito è soddisfatto se la memoria è persistente tra sessioni (finché la directory dati lato server non viene azzerata o cancellata) e se due avatar distinti non condividono documenti indicizzati; l'isolamento può essere verificato confrontando i risultati di \texttt{/recall} a parità di query.

RF5 (Turno vocale end-to-end in push-to-talk): il client deve gestire una conversazione a turni acquisendo audio, inviandolo allo STT con \texttt{/transcribe}, inoltrando la trascrizione al RAG con \texttt{/chat} e riproducendo la risposta audio tramite streaming con \texttt{/tts\_stream}. Il requisito è soddisfatto se, per ogni turno, il sistema produce testo trascritto, testo di risposta e avvio del playback audio con transizioni UI coerenti tra ascolto, elaborazione e riproduzione.

RF6 (Ingest multimodale per memoria per-avatar): il sistema deve consentire l'ingestione multimodale nella memoria dell'avatar, includendo descrizione immagine con \texttt{/describe\_image} e ingestione documentale con \texttt{/ingest\_file} (estrazione e indicizzazione del contenuto). Il requisito è soddisfatto se, dopo l'operazione, i contenuti risultano recuperabili tramite \texttt{/recall} e/o se \texttt{/avatar\_stats} riporta \texttt{has\_memory=true} per l'avatar.

\subsection{Requisiti non funzionali}

I requisiti non funzionali sono formulati secondo il modello ISO/IEC 25010 e si concentrano sulle caratteristiche che influenzano la plausibilità dell'interazione vocale e la sostenibilità tecnica del prototipo. In VR la specifica dei requisiti richiede spesso di dettagliare flussi di scena, artefatti e comportamenti; inoltre cambiamenti minimi possono amplificare costi e complessità. Per questo conviene fissare vincoli di qualità verificabili già in fase architetturale \cite{Karre2024VReqST}.

RNF1 (Efficienza prestazionale - time behaviour): la latenza percepita deve restare controllata misurando il tempo tra rilascio del comando push-to-talk e primo campione audio riprodotto dal TTS. Per il prototipo si adotta come soglia prestazionale una risposta udibile entro 12 s in esecuzione locale e entro 16 s in deploy pubblico. Il target è calibrato sulle misure osservate di STT+TTS e include un margine operativo per la fase RAG/LLM, non misurata sistematicamente per singolo stadio. Questa metrica non coincide con il tempo di cold-start iniziale dei servizi. La conformità al requisito è verificata tramite timestamp lato client sui turni vocali, come nella metodologia di misura riportata nel Capitolo~\ref{chap:risultati}, calcolando percentuali di turni entro soglia e percentili di latenza nei due contesti (locale/pubblico). Approcci a micro-servizi per agenti sociali real-time trattano la latenza come vincolo primario e riportano tempi dell'ordine di pochi secondi fino alla prima emissione vocale in pipeline ottimizzate \cite{Lin2024Estuary}.

RNF2 (Affidabilità - availability e fault tolerance): durante una sessione di 60 minuti il sistema deve completare con successo tra il 95\% e il 97\% dei turni vocali, dove un turno è riuscito se STT, \texttt{/chat} e \texttt{/tts\_stream} restituiscono esito valido entro timeout. Sono implementati timeout, retry limitati a \texttt{retryCount} (configurabile) e gestione esplicita degli errori lato client; in caso di fallimento il flusso degrada su messaggi di stato UI senza blocchi permanenti. La conformità al requisito è determinata conteggiando turni riusciti e falliti nei log di sessione.

RNF3 (Manutenibilità - modularity e portabilità): la sostituzione di un micro-servizio deve richiedere solo una variazione di configurazione centralizzata lato client (base URL, timeout e policy di retry), mantenendo invariati endpoint e payload attesi. Il requisito si considera soddisfatto quando la stessa build Unity WebGL funziona sia in locale (URL assoluti su loopback) sia in deploy dietro reverse proxy con path \texttt{/api/<servizio>}, e quando dopo un aggiornamento o un redeploy viene rieseguita una checklist minima di compatibilità (\texttt{/health} e una chiamata funzionale per ciascun servizio).

RNF4 (Coerenza estetica e comunicazione di stato): il linguaggio visivo deve restare coerente con la natura stilizzata degli avatar Avaturn e con il carattere sperimentale del prototipo. Nel client questo vincolo si traduce in effetti di post-processing, rings animati e feedback di selezione, scelti perché compatibili con rendering WebGL e utili a rendere leggibili gli stati operativi senza aumentare il carico cognitivo dell'utente. Il requisito è soddisfatto se gli indicatori visivi accompagnano in modo consistente transizioni e operazioni principali dell'interfaccia (inizializzazione, setup, onboarding, selezione), mantenendo continuità tra resa dell'avatar e contesto UI.

\section{Architettura di riferimento di SOULFRAME}
\label{sec:architettura-riferimento}
\subsection{Vista d'insieme dei componenti frontend/backend}
SOULFRAME adotta un'architettura client--server a componenti, progettata per mantenere sul browser le responsabilità sensibili al frame-rate (interfaccia e resa 3D) e delegare al backend i compiti di inferenza e persistenza. Il client è una build Unity WebGL eseguita nel browser: gestisce la UI, le transizioni di stato dell'esperienza e l'interazione push-to-talk, oltre al rendering e alla visualizzazione dell'avatar. La comunicazione verso il backend avviene via HTTP e viene normalizzata tramite una configurazione centralizzata che astrae la differenza tra esecuzione locale (URL e porte esplicite su loopback) e deploy pubblico (path relativi sotto un unico origin).

Sul backend, la pipeline è scomposta in quattro micro-servizi FastAPI indipendenti, ciascuno con un contratto API mirato. Whisper (porta 8001) espone \texttt{/transcribe} e trasforma l'audio in testo. In locale Windows il profilo predefinito è \texttt{small}; nel setup Ubuntu testato il profilo usato è \texttt{medium}, grazie a una VRAM disponibile maggiore (circa 24 GB contro circa 12 GB in locale). Il servizio RAG (porta 8002) governa l'orchestrazione conversazionale con \texttt{/chat}, insieme alla memoria per-avatar (\texttt{/remember}, \texttt{/recall}) e all'ingestione di contenuti (\texttt{/ingest\_file}). In questa architettura il RAG è il punto di convergenza: costruisce il contesto via retrieval e delega a un runtime esterno le operazioni di Large Language Model (LLM) ed embedding. Il servizio Text-to-Speech (TTS), basato su Coqui XTTS v2 \cite{Casanova2024XTTS} e in ascolto sulla porta 8004, espone sintesi completa (\texttt{/tts}), sintesi in streaming (\texttt{/tts\_stream}) e endpoint per frasi di attesa (\texttt{/wait\_phrase}, \texttt{/generate\_wait\_phrases}). L'Avatar Asset Server (porta 8003) gestisce lista e import degli avatar (\texttt{/avatars/list}, \texttt{/avatars/import}) e serve i modelli \texttt{.glb} tramite \texttt{/avatars/{id}/model.glb}, mantenendo persistente la cache degli asset tra sessioni.

Il servizio di supporto Ollama opera come runtime separato per LLM ed embeddings sulla porta 11434. Nel prototipo il profilo del modello di chat varia tra ambiente locale Windows e deploy Ubuntu: in locale è adottata la variante \texttt{llama3:8b-instruct-q4\_K\_M}, mentre sul server Ubuntu la configurazione di default è \texttt{llama3.1:8b}; il modello embedding resta invece dedicato al retrieval. L'uso di Ollama come runtime separato mantiene il micro-servizio RAG focalizzato sull'orchestrazione applicativa (memoria, retrieval e composizione della richiesta) e consente di sostituire o aggiornare i modelli senza modificare il client, a patto di preservare endpoint e formati attesi. I dettagli implementativi di questa parametrizzazione sono discussi in \autoref{subsec:rag-memory}.

In deploy pubblico, l'elemento chiave di integrazione è il reverse proxy Caddy, che svolge due funzioni: terminazione TLS e punto d'ingresso unico per il browser. Il client WebGL comunica con un solo origin HTTPS e invoca le API tramite path riscritti (\texttt{/api/whisper/*}, \texttt{/api/rag/*}, \texttt{/api/avatar/*}, \texttt{/api/tts/*}); Caddy inoltra le richieste alle porte interne 8001--8004, mantenute non esposte verso l'esterno. Il reverse proxy riduce la complessità lato browser (stessa origine, routing consistente) e isola i servizi, che restano indirizzabili e gestibili separatamente a livello di processo. Nel progetto Caddy è stato preferito a Nginx per tre motivi operativi: TLS automatico, redirect HTTP$\rightarrow$HTTPS integrato e configurazione più compatta in fase di bootstrap. Nel prototipo, gli endpoint non implementano autenticazione/autorizzazione applicativa e l'architettura assume una rete di esecuzione controllata. Figura~\ref{fig:arch-cap3} riassume questa vista a blocchi, chiarendo sia la separazione di responsabilità tra frontend e backend sia il ruolo del proxy come snodo di comunicazione e sicurezza.

Per una vista sintetica della catena conversazionale completa, Figura~\ref{fig:pipeline-e2e} mostra il flusso end-to-end dall'input vocale alla risposta dell'agente embodied.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{pipeline_e2e.png}
\caption{Flusso end-to-end del sistema SOULFRAME: dall'input vocale dell'utente alla risposta dell'agente embodied.}
\label{fig:pipeline-e2e}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{arch_cap3.png}
\caption{Vista d'insieme dell'architettura a componenti di SOULFRAME: il client Unity WebGL comunica con i micro-servizi backend attraverso il reverse proxy Caddy.}
\label{fig:arch-cap3}
\end{figure}

Operativamente, la stessa scomposizione in componenti viene mantenuta sia in locale sia su server. In ambiente Windows lo script di avvio coordina l'esecuzione dei servizi e applica controlli di base (ad esempio la disponibilità delle porte) per ridurre conflitti tra processi durante lo sviluppo. In ambiente Ubuntu, ciascun micro-servizio è gestito come unità \texttt{systemd} (systemd: gestore di processi e servizi del sistema operativo Linux, responsabile dell'avvio automatico e del restart dei servizi), con comandi amministrativi che permettono start/stop/status e aggiornamenti senza dover riconfigurare manualmente l'intero stack. Questa separazione è rimasta invariata durante lo sviluppo perché i colli di bottiglia più difficili da diagnosticare emergevano tra servizi diversi, non dentro un singolo modulo.

\subsection{\texorpdfstring{Flusso end-to-end audio $\rightarrow$ testo $\rightarrow$ risposta $\rightarrow$ audio}{Flusso end-to-end audio -> testo -> risposta -> audio}}
Il flusso conversazionale end-to-end di SOULFRAME è organizzato come una pipeline a turni che parte dall'input vocale dell'utente e termina con la riproduzione della risposta sintetizzata, mantenendo sul client Unity WebGL le responsabilità di interazione e rendering e delegando al backend le fasi di inferenza. L'interazione è di tipo push-to-talk: l'utente tiene premuto \texttt{SPACE} per parlare e, al rilascio, il client chiude la registrazione e prepara un file audio in formato WAV, includendo un parametro di lingua nella richiesta. In ambiente WebGL l'acquisizione del microfono e la gestione del contesto audio del browser richiedono un bridge JavaScript, qui realizzato tramite un plugin (\texttt{AudioCapture.jslib}) richiamato dal componente di registrazione in Unity, così da appoggiarsi alle primitive audio disponibili nel runtime WebGL.\footnote{Unity Technologies, \emph{Audio in WebGL}, \url{https://docs.unity3d.com/6000.2/Documentation/Manual/webgl-audio.html}}

Una volta completata la cattura, il client invia l'audio al servizio Speech-to-Text (STT) basato su Whisper tramite una richiesta \texttt{POST /transcribe}. Il micro-servizio esegue la trascrizione e restituisce un JSON che contiene il testo riconosciuto nel campo \texttt{"text"}. L'isolamento dell'Automatic Speech Recognition (ASR) in un componente dedicato rende esplicita la prima trasformazione del flusso, da segnale audio (WAV) a contenuto testuale (stringa), su cui è poi possibile applicare logiche conversazionali e di memoria.

Il servizio RAG/LLM riceve quindi la trascrizione tramite una richiesta \texttt{POST /chat} che include l'identificativo dell'avatar (\texttt{avatar\_id}). In questo stadio il backend svolge la funzione di orchestratore: recupera contesto dalla memoria per-avatar, costruita su un database vettoriale persistente (ChromaDB\footnote{Chroma, \emph{Chroma Documentation}, \url{https://docs.trychroma.com/}}) e popolata tramite embedding generati con un modello dedicato (nella configurazione corrente \texttt{nomic-embed-text} erogato da Ollama). La scelta di ChromaDB privilegia leggerezza e semplicità di esecuzione locale (senza servizi esterni o account) e si integra in modo diretto con la gerarchia per-avatar \texttt{rag\_store/<avatar\_id>/}; alternative come FAISS possono offrire throughput maggiore, ma richiedono una gestione più artigianale della persistenza e del layout per profilo nel contesto di questo prototipo. Il retrieval, quando disponibile memoria, produce un insieme di passaggi contestuali che vengono integrati nel prompt e usati per vincolare e arricchire la generazione. Il servizio invoca poi il modello linguistico per la risposta (nella configurazione corrente \texttt{llama3:8b-instruct-q4\_K\_M} via Ollama), ottenendo un testo finale che viene restituito al client come contenuto della risposta. In questa fase si concentrano sia la dipendenza dalla memoria dell'avatar sia la variabilità computazionale dovuta alla generazione, motivo per cui la gestione della latenza percepita diventa parte integrante del disegno architetturale.

Ricevuto il testo di risposta, il client attiva la sintesi vocale tramite il servizio Text-to-Speech (TTS) basato su Coqui XTTS v2. La richiesta avviene preferibilmente tramite l'endpoint di streaming \texttt{/tts\_stream}, includendo \texttt{avatar\_id} e lingua: il servizio recupera il profilo vocale associato all'avatar (voice cloning) e produce audio progressivamente, consentendo al client di iniziare il playback non appena arrivano i primi byte dello stream. Lo streaming riduce il tempo tra fine turno utente e inizio della risposta udibile, perché sposta l'attesa dal completamento dell'intero file audio alla sola generazione dell'incipit, migliorando la continuità percepita anche quando la risposta è lunga.

SOULFRAME introduce inoltre strategie specifiche per mitigare i tempi morti più evidenti. Per contenere la latenza iniziale, il servizio TTS adotta meccanismi di warmup durante il boot; in parallelo, il frontend mostra un pannello di caricamento dedicato e rende disponibili le modalità operative solo quando il TTS risulta pronto. Lo stesso problema può riemergere durante i turni successivi, quando la pipeline STT$\rightarrow$RAG/LLM$\rightarrow$TTS è in corso: in questa fase il sistema usa un fallback audio di cortesia per evitare silenzi prolungati e dare un segnale immediato di reattività. I dettagli implementativi di warmup, wait phrases e streaming sono discussi nel Capitolo~\ref{chap:implementazione}.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{ui_state_diagram_cap3.png}
\caption{Diagramma degli stati UI del client SOULFRAME: dalla fase
di caricamento iniziale al ciclo conversazionale
(Idle $\rightarrow$ Listening $\rightarrow$ Processing $\rightarrow$ Speaking)
e alla gestione degli errori.}
\label{fig:ui-state-diagram-cap3}
\end{figure}

Figura~\ref{fig:flusso-e2e-cap3} sintetizza la sequenza temporale e i formati dati scambiati tra componenti, mettendo in evidenza dove avvengono le trasformazioni principali (WAV$\rightarrow$testo, testo$\rightarrow$testo contestualizzato, testo$\rightarrow$audio) e dove si innestano le tecniche di riduzione della latenza percepita. La stessa figura chiarisce che, in deploy con reverse proxy, gli endpoint REST restano invariati a livello logico e vengono raggiunti tramite path riscritti sotto un unico origin HTTPS.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{flusso_e2e_cap3.png}
\caption{Flusso end-to-end di una richiesta conversazionale in SOULFRAME: dall'acquisizione audio push-to-talk alla riproduzione della risposta vocale.}
\label{fig:flusso-e2e-cap3}
\end{figure}

\subsection{Flusso di gestione avatar (creazione, import, cache, rendering)}
Il flusso di gestione avatar in SOULFRAME affianca due percorsi che convergono nella stessa esperienza in scena: avatar locali pre-inclusi nella build e avatar importati dall'utente. L'obiettivo architetturale è garantire sempre una libreria minima selezionabile, mantenendo allo stesso tempo un canale controllato per acquisire e servire asset esterni in formato \texttt{.glb}.

Nel percorso locale, il client seleziona un profilo già disponibile e lo carica direttamente in scena. Nel percorso importato, il profilo viene creato esternamente, importato dal backend e poi reso disponibile nella stessa libreria del client. In entrambi i casi, il punto di convergenza resta identico: selezione del profilo, caricamento del modello e attivazione dell'avatar nella pipeline conversazionale.

Dopo selezione o creazione, il flusso prosegue con onboarding voce e memoria per-avatar. L'accesso a \texttt{MainMode} avviene solo quando i prerequisiti minimi del profilo risultano soddisfatti, così da evitare conversazioni prive di contesto operativo. I dettagli implementativi del bridge WebGL, del ciclo di import e delle logiche di robustezza lato asset sono trattati nel Capitolo~\ref{chap:implementazione}.

\begin{figure}[t]
\centering
\includegraphics[width=0.88\textwidth]{flusso_avatar_cap3.png}
\caption{Flusso di gestione avatar in SOULFRAME: dalla creazione tramite Avaturn alla cache server-side e al rendering nel client Unity.}
\label{fig:flusso-avatar-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_carosello.png}
\caption{Interfaccia carosello avatar in SOULFRAME: selezione del
profilo attivo tra avatar locali e importati.}
\label{fig:ui-carosello-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_mainmenu.png}
\caption{Interfaccia \texttt{MainMenu} in SOULFRAME: accesso alle funzioni principali prima dell'onboarding dell'avatar.}
\label{fig:ui-mainmenu-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_setupvoice.png}
\caption{Schermata \texttt{SetupVoice}: registrazione e validazione del campione vocale durante l'onboarding.}
\label{fig:ui-setupvoice-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_setupmemory.png}
\caption{Schermata \texttt{SetupMemory}: inserimento note e ingestione contenuti per popolare la memoria per-avatar.}
\label{fig:ui-setupmemory-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_push_to_talk.png}
\caption{Stato di ascolto push-to-talk nel client Unity WebGL:
acquisizione audio attiva con indicatore visivo.}
\label{fig:ui-ptt-cap3}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{ui_mainmode.png}
\caption{MainMode conversazionale in SOULFRAME: trascrizione e
risposta dell'avatar in corso durante un turno vocale.}
\label{fig:ui-mainmode-cap3}
\end{figure}

\section{Componenti implementati}
I componenti implementati rendono operativa l'architettura della Sezione~\ref{sec:architettura-riferimento}: integrazione Avaturn nel client Unity WebGL, backend AI a micro-servizi e persistenza per profilo avatar. La panoramica unificata dei servizi backend è riportata in Tabella~\ref{tab:microservizi-cap3}; i dettagli fini di funzioni e ottimizzazioni sono nel Capitolo~4.

\subsection{Integrazione Avaturn nel frontend Unity WebGL}
L'integrazione Avaturn nel frontend Unity WebGL segue una logica a due passaggi: creazione del profilo avatar in ambiente web dedicato e successivo import del modello \texttt{.glb} nel flusso applicativo di SOULFRAME. A livello architetturale conta il punto di convergenza: una volta completata la creazione, il client acquisisce l'identità dell'avatar e delega l'import al backend, riallineandosi poi al percorso unico descritto in Sezione~\ref{sec:architettura-riferimento}. Nel runtime Unity il caricamento dei \texttt{.glb} avviene con GLTFast\footnote{Unity Technologies, \emph{glTFast package documentation}, \url{https://docs.unity3d.com/Packages/com.unity.cloud.gltfast@6.10/manual/index.html}}. I dettagli implementativi del bridge WebGL, della gestione iframe/DOM e dell'interoperabilità C\#/JavaScript sono trattati nel Capitolo~\ref{chap:implementazione} (\autoref{subsec:avaturn-webview}).

\subsection{Backend AI a micro-servizi (Whisper, RAG, TTS, Avatar Asset)}
Il backend di SOULFRAME è realizzato come insieme di micro-servizi indipendenti basati su FastAPI, avviati con \texttt{uvicorn} su porte dedicate (8001--8004). La scomposizione in servizi rende esplicita la separazione delle responsabilità e permette di verificare disponibilità e corretto instradamento con controlli semplici (\texttt{/health}) prima di abilitare il flusso conversazionale. Tabella~\ref{tab:microservizi-cap3} sintetizza i quattro componenti e i relativi contratti API, collegando porte, endpoint e responsabilità operative in un'unica vista.

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\raggedright\arraybackslash}p{0.16\textwidth}|>{\raggedright\arraybackslash}p{0.20\textwidth}|>{\raggedright\arraybackslash}p{0.07\textwidth}|>{\raggedright\arraybackslash}p{0.24\textwidth}|>{\raggedright\arraybackslash}p{0.27\textwidth}|}
\hline
\textbf{Componente} & \textbf{File Python} & \textbf{Porta} & \textbf{Endpoint chiave} & \textbf{Responsabilità} \\
\hline
Whisper STT & \path{whisper_server.py} & \texttt{8001} & \path{GET /health}\ \path{POST /transcribe} & Trascrizione Speech-to-Text (STT) di audio caricato (WAV o formati compatibili), gestione lingua e cleanup di file temporanei. \\
\hline
RAG/LLM & \path{rag_server.py} & \texttt{8002} & \path{GET /health}\ \path{GET /avatar_stats}\ \path{POST /chat}\ \path{POST /remember}\ \path{POST /recall}\ \path{POST /ingest_file}\ \path{POST /describe_image} & Orchestrazione Retrieval-Augmented Generation (RAG): memoria per-avatar su ChromaDB, retrieval ibrido (similarità vettoriale + BM25), chiamate a Ollama per LLM/embedding e ingestione multimodale con estrazione/indicizzazione dei contenuti. \\
\hline
Coqui XTTS TTS & \path{coqui_tts_server.py} & \texttt{8004} & \path{GET /health}\ \path{POST /tts}\ \path{POST /tts_json}\ \path{POST /set_avatar_voice}\ \path{GET/DELETE /avatar_voice}\ \path{POST /generate_wait_phrases}\ \path{GET /wait_phrase} & Sintesi Text-to-Speech (TTS) con voice cloning per avatar, streaming audio e generazione/serving di frasi di attesa; gestione e persistenza dei profili vocali per \texttt{avatar\_id}. \\
\hline
Avatar Asset Server & \path{avatar_asset_server.py} & \texttt{8003} & \path{GET /health}\ \path{GET /avatars/list}\ \path{POST /avatars/import}\ \path{GET /avatars/{id}/model.glb}\ \path{DELETE /avatars/{id}} & Import e caching server-side di modelli \texttt{.glb}, deduplicazione via hash URL, self-healing dei metadati e lista unificata di avatar locali di fallback e avatar importati. \\
\hline
\end{tabular}
\caption{Riepilogo dei micro-servizi backend di SOULFRAME: porte, endpoint principali e responsabilità operative.}
\label{tab:microservizi-cap3}
\end{table}

Whisper presidia la trasformazione audio$\rightarrow$testo e costituisce il punto di ingresso del canale vocale utente. Il servizio RAG/LLM governa memoria per-avatar e generazione contestuale, coordinando retrieval e risposta conversazionale. Il servizio TTS converte la risposta in audio, riusando profili vocali per-avatar e gestendo le frasi di attesa. L'Avatar Asset Server isola il ciclo di vita dei modelli \texttt{.glb} (import, cache e distribuzione) dal resto della pipeline conversazionale. In questa sottosezione, il termine \emph{ricerca ibrida} indica la combinazione di similarità vettoriale e BM25 nella fase di retrieval del RAG; nel prototipo il bilanciamento 0.6/0.4 favorisce leggermente BM25 perché le query utente contengono spesso keyword e nomi propri, mantenendo comunque una quota semantica utile per recuperare ricordi parafrasati. In scenari diversi restano praticabili varianti 0.5/0.5 (embedding più stabili) o 0.7/0.3 (maggiore controllo sul semantic drift).

La scomposizione in servizi autonomi mantiene il client Unity disaccoppiato dalla logica interna di inferenza e storage, rendendo più semplice evolvere singoli componenti senza cambiare il flusso utente. Questo vincolo consente di aggiornare STT, RAG o TTS in modo indipendente durante i test comparativi. I dettagli implementativi (caricamento modelli, gestione file temporanei, chunking/overlap, estrazione contenuto, deduplicazione, self-healing e parametri runtime) sono discussi nel Capitolo~\ref{chap:implementazione}.
In Figura~\ref{fig:rag-swagger-cap3} è riportata la documentazione automatica degli endpoint.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{rag_docs_swagger.png}
\caption{Interfaccia Swagger UI (\texttt{/docs}) del servizio RAG di
SOULFRAME: gli endpoint esposti da \texttt{rag\_server.py} con
schema automatico generato da FastAPI.}
\label{fig:rag-swagger-cap3}
\end{figure}

\subsection{Persistenza e gestione dati per avatar}

Nel sistema, la persistenza dei dati in SOULFRAME è organizzata su filesystem e segue una separazione per \texttt{avatar\_id}: ogni profilo mantiene in modo indipendente tre store dedicati, ovvero \texttt{avatar\_store/} per gli asset \texttt{.glb}, \texttt{voices/avatars/} per il profilo vocale e \texttt{rag\_store/} per la memoria conversazionale. Questa struttura mantiene allineati identità dell'avatar e risorse operative lungo tutto il ciclo di vita del profilo.

\begin{figure}[t]
\centering
\includegraphics[width=0.75\textwidth]{persistenza_tree_cap3.png}
\caption{Struttura del filesystem di persistenza in SOULFRAME:
tre store isolati per \texttt{avatar\_id} (asset \texttt{.glb},
profilo vocale e memoria RAG).}
\label{fig:persistenza-tree-cap3}
\end{figure}

L'isolamento per profilo consente di evitare interferenze tra avatar distinti e di ricostruire lo stato dopo riavvio rileggendo i rispettivi percorsi persistenti. A livello applicativo, la presenza di memoria può essere verificata tramite \texttt{/avatar\_stats} (campo \texttt{has\_memory}), mentre asset e voce restano associati allo stesso \texttt{avatar\_id} nei rispettivi store.

I dettagli implementativi (deduplicazione, scrittura atomica, self-healing dei metadati, generazione lazy delle frasi di attesa) sono discussi nel Capitolo~\ref{chap:implementazione}.

\section{Setup e deploy operativo}
Il setup e il deploy operativo rendono concreta l'architettura descritta nella Sezione~\ref{sec:architettura-riferimento}, traducendo la scomposizione in componenti in procedure ripetibili di avvio, arresto e verifica. SOULFRAME mantiene la stessa struttura logica in locale e in produzione, ma cambia l'orchestrazione dei processi e il punto di ingresso di rete, in coerenza con i vincoli di portabilità e manutenibilità discussi in Sezione~\ref{sec:requisiti}. Per ridurre errori di esercizio, ho privilegiato passaggi operativi ripetibili e un punto di controllo unificato per i servizi.

\subsection{Ambiente locale Windows}
In ambiente locale la configurazione è orientata a sviluppo e test rapidi: i servizi girano su loopback e la build Unity WebGL è servita in HTTP. Il provisioning iniziale è automatizzato da \texttt{setup\_soulframe\_windows.bat}, che prepara ambiente Python, dipendenze backend e configurazioni base necessarie all'avvio del prototipo. In questa modalità l'obiettivo non è la hardening dell'infrastruttura, ma la riduzione del tempo tra modifica del codice e verifica end-to-end.

L'avvio operativo è centralizzato in \texttt{ai\_services.cmd}, che coordina i processi applicativi, riduce i conflitti su porte già occupate e applica una sequenza di start coerente con le dipendenze della pipeline. In pratica, lo sviluppatore mantiene un punto unico per start/stop/restart dello stack locale, evitando avvii manuali separati dei micro-servizi.

Questo assetto favorisce debugging iterativo e osservabilità immediata: i log dei servizi sono disponibili in console, gli endpoint risultano interrogabili direttamente su loopback e il client WebGL può essere validato senza introdurre variabili infrastrutturali esterne. Le differenze operative rispetto al deploy server sono sintetizzate in Tabella~\ref{tab:deploy-confronto-cap3}.

\begin{table}[t]
\centering
\footnotesize
\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|>{\raggedright\arraybackslash}p{0.19\textwidth}|>{\raggedright\arraybackslash}p{0.35\textwidth}|>{\raggedright\arraybackslash}p{0.35\textwidth}|}
\hline
\textbf{Aspetto} & \textbf{Windows locale} & \textbf{Ubuntu server} \\
\hline
Avvio servizi & Script locale (\texttt{ai\_services.cmd}) con processi separati in ascolto su loopback & Service manager con unità dedicate e restart automatico \\
\hline
Origin/URL usati dal client & Origin locale (\texttt{http://localhost:8000}) con porte backend esplicite in configurazione client & Origin HTTPS unico; API raggiunte via path \texttt{/api/<servizio>} dietro reverse proxy \\
\hline
TLS & Assente nel loop di sviluppo locale (HTTP) & Terminazione TLS al reverse proxy \\
\hline
Logging & Log di console/processo per debugging interattivo & Log centralizzati di servizi e reverse proxy \\
\hline
Update/rollback & Aggiornamento manuale dello workspace e riavvio script & Script amministrativi con stop/start orchestrato e supporto backup/rollback \\
\hline
Gestione persistenze & Directory locali del progetto (\texttt{avatar\_store}, \texttt{voices}, \texttt{rag\_store}) & Directory persistenti lato server con separazione per componente/avatar \\
\hline
\end{tabular}
\caption{Confronto operativo tra setup locale Windows e deploy server Ubuntu.}
\label{tab:deploy-confronto-cap3}
\end{table}

\subsection{Ambiente server Ubuntu}
In ambiente Ubuntu il deploy è orientato all'esposizione pubblica controllata del prototipo. I micro-servizi restano su rete interna, mentre il browser raggiunge un unico origin HTTPS tramite reverse proxy. Questa scelta semplifica l'integrazione lato client (stessa origine, path applicativi uniformi) e riduce la superficie di esposizione diretta delle singole porte interne.

Il setup server organizza artefatti e configurazione runtime in percorsi dedicati, separando componenti applicativi, file statici WebGL e variabili ambientali operative. La gestione dei processi è demandata a unità del service manager con politiche di restart, così che il ripristino dopo reboot o failure non richieda intervento manuale su ogni micro-servizio.

Nel routing, il reverse proxy gestisce terminazione TLS, serving dei file statici e inoltro delle richieste API verso i servizi interni. La stessa build Unity WebGL resta così riutilizzabile tra locale e server, variando soprattutto il livello di orchestrazione e il punto di ingresso di rete. I dettagli puntuali di provisioning e parametrizzazione sono rinviati al Capitolo~\ref{chap:implementazione}.

\subsection{Servizi di supporto (systemd, Caddy, script amministrativi)}
Accanto ai micro-servizi applicativi, il prototipo include un livello di supporto operativo che copre tre esigenze: lifecycle dei processi, gateway/reverse proxy e automazione amministrativa. Questo livello mantiene ripetibile la gestione dell'ambiente, soprattutto quando la piattaforma viene aggiornata o riavviata frequentemente.

Il service manager fornisce avvio al boot, restart automatico e consultazione unificata dei log, consentendo interventi mirati su singoli servizi o sull'intero stack. Il reverse proxy concentra invece le responsabilità di ingress (TLS, routing API e serving statico), così da mantenere il browser disaccoppiato dai dettagli di rete interni.

Gli script amministrativi completano il quadro con procedure standardizzate di manutenzione (stato, restart, aggiornamento, backup e rollback), riducendo il rischio di azioni manuali non ripetibili. La tabella comparativa precedente rende verificabili queste differenze tra ambienti; i dettagli esecutivi sono approfonditi nel Capitolo~\ref{chap:implementazione}.
