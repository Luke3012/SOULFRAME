\chapter{Fondamenti e Stato dell'Arte}
\label{chap:stato-arte}

\section{Agenti conversazionali embodied in XR}
\label{sec:eca-xr}
Gli Embodied Conversational Agents (ECA) sono agenti conversazionali dotati di una rappresentazione corporea, progettati per essere percepiti come interlocutori nello spazio di interazione. In Extended Reality (XR), il corpo virtuale viene collocato in una scena tridimensionale e coordinato con i turni del dialogo per rendere l'interazione più naturale rispetto a un'interfaccia solo testuale.

La letteratura recente mostra che molti sistemi ECA in XR sono sviluppati in Virtual Reality (VR), spesso con Head-Mounted Display (HMD) e con Unity come piattaforma ricorrente. Sono frequenti anche l'interazione vocale e le configurazioni uno-a-uno. Molte applicazioni restano orientate a compiti specifici, ma cresce l'interesse verso dialoghi più adattivi supportati da modelli neurali.\cite{Yang2025ECA_XR} SOULFRAME si colloca in questo scenario come ECA vocale embodied in un ambiente 3D fruibile via WebGL senza HMD: l'agente conversa in modalità push-to-talk e mantiene un profilo avatar che integra aspetto e voce.

La traiettoria che porta dai CA scriptati ai sistemi basati su modelli linguistici è sintetizzata in Figura~\ref{fig:ca-evoluzione}. Il passaggio chiarisce perché embodiment e gestione del dialogo vengano oggi trattati insieme nei prototipi XR.

\begin{figure}[t]
\centering
% LINK_ORIGINE: [https://www.researchgate.net/figure/The-five-identified-waves-of-CA-evolutions-as-identified-by-Schobel-et-al-8_fig1_391468212]
\includegraphics[width=0.85\textwidth]{vr_contesto_schema.png}
\caption{Evoluzione della maturità della ricerca sui Conversational Agents (CA), dalla ``zero hour wave'' alla ``AI wave'', con riferimento ai principali passaggi tecnologici. \protect\footnotemark}
\label{fig:ca-evoluzione}
\end{figure}
\footnotetext{Fonte: adattamento da una figura pubblicata su ResearchGate.}

\subsection{Presenza sociale, co-presenza e ruolo della voce}
Quando l'interazione avviene in un ambiente mediato, la presenza sociale descrive la percezione dell'altro come interlocutore con cui si condivide un'esperienza. In valutazione conviene scomporre questo fenomeno in componenti distinguibili, includendo la co-presenza (percezione di condividere lo stesso spazio) e forme di coinvolgimento attentivo e comportamentale che rendono il dialogo più contingente e reciproco.\cite{Oh2018SocialPresence}

La voce è un canale rilevante per la presenza sociale perché rende immediatamente percepibili tempi di risposta, ritmo e prosodia. In VR questo effetto risulta più marcato quando l'agente è embodied: evidenze sperimentali indicano che un ECA con interazione vocale in tempo reale (STT+TTS) aumenta la co-presenza percepita rispetto a una condizione con audio pre-registrato.\cite{Kan2023ECA_Training} In SOULFRAME, la scelta del push-to-talk e l'uso di una voce persistente per avatar seguono questa logica, con l'obiettivo di stabilizzare i turni e mantenere coerenza d'identità.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{social_presence_audio_quality.png}
\caption{Fattori immersivi associati alla presenza sociale, con qualità audio tra le variabili considerate in letteratura. \protect\footnotemark}
\label{fig:social-presence-audio}
\end{figure}
\footnotetext{Fonte: Oh et al., \emph{A Systematic Review of Social Presence: Definition, Antecedents, and Implications}, Frontiers in Robotics and AI (2018), \url{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00114/full} (Figura 3; licenza/uso: CC BY).}

La presenza sociale e la co-presenza dipendono dall'allineamento tra canali comunicativi e tempi dell'interazione. In Figura~\ref{fig:social-presence-audio} la qualità audio compare tra le variabili immersive considerate in letteratura. Per un ECA vocale, qualità della voce e gestione temporale dei turni influenzano la naturalezza percepita; in SOULFRAME, push-to-talk, profilo vocale per avatar, warmup e frasi di attesa mitigano sovrapposizioni e silenzi durante l'elaborazione.

Nel contesto delle interazioni mediate conviene distinguere loneliness e isolamento sociale oggettivo. L'isolamento riguarda la quantità dei contatti; la loneliness riguarda invece come quei legami vengono vissuti. Nasce da una discrepanza percepita tra le relazioni che una persona desidera e quelle che sente di avere. Per questo motivo non coincide con il semplice ``stare da soli'': si può avere una rete di contatti e sentirsi comunque disconnessi.\cite{HawkleyCacioppo2010}

Quando questa condizione diventa cronica, le conseguenze sono rilevanti: aumento del rischio depressivo, declino cognitivo, maggiore vulnerabilità cardiovascolare e incremento della mortalità prematura (circa 26\%).\cite{HawkleyCacioppo2010} La letteratura descrive un circuito auto-rinforzante: la percezione di disconnessione aumenta l'ipervigilanza verso possibili minacce sociali, che a sua volta favorisce letture negative delle interazioni e indebolisce l'autoregolazione. Nel tempo, questo meccanismo rende più probabile confermare la stessa percezione iniziale di esclusione.

Per gli ECA embodied questo non implica una promessa terapeutica, ma chiarisce un problema applicativo reale. Se la loneliness dipende anche da come viene percepita la qualità della relazione, un agente che mantiene continuità tra turni, coerenza vocale e memoria può offrire un'interazione più stabile rispetto a una UI testuale stateless. In questa lettura, la social presence resta centrale: percepire un interlocutore come ``presente'' nello stesso spazio comunicativo cambia il modo in cui il dialogo viene interpretato.\cite{Oh2018SocialPresence}

In questa cornice si colloca anche la scelta di SOULFRAME: trattare l'avatar come profilo operativo completo (aspetto, voce e memoria), non come semplice canale di output testuale. L'obiettivo non è ``curare'' la loneliness né sostituire relazioni umane, ma ridurre incoerenze che interrompono la continuità percepita del dialogo. Per questo, robustezza della pipeline e coerenza del profilo conversazionale restano prerequisiti sia tecnici sia valutativi.

\subsection{Limiti aperti nei sistemi conversazionali immersivi}
L'integrazione del dialogo in ambienti 3D introduce criticità che emergono meno nei sistemi testuali tradizionali. La principale è la latenza end-to-end tra acquisizione audio, trascrizione, generazione e sintesi: ritardi percepibili riducono la naturalezza percepita anche quando il contenuto della risposta è corretto.

La continuità tra turni è un secondo limite: senza una memoria affidabile il sistema fatica a mantenere riferimenti e preferenze espresse dall'utente. Anche l'integrazione multimodale resta critica, perché richiede coerenza tra risposta linguistica, tempi e segnali non verbali. La valutazione, infine, è complessa: combina metriche soggettive (presenza, co-presenza, naturalezza) e metriche tecniche (latenza, errori di trascrizione), con protocolli non sempre uniformi.

In letteratura, architetture modulari open-source per ECA vocali in VR mostrano che la separazione in servizi STT e TTS semplifica l'integrazione ma rende evidenti i colli di bottiglia temporali, richiedendo strategie come output in streaming.\cite{Yin2023VoiceVR} SOULFRAME adotta una pipeline a tre stadi, memoria persistente per avatar e frasi di attesa per ridurre l'impatto dei tempi morti senza dipendenze cloud.

\section{Pipeline AI adottata in SOULFRAME}
\label{sec:pipeline-ai}
SOULFRAME implementa una pipeline conversazionale in tre stadi: riconoscimento del parlato, generazione contestuale e sintesi vocale. Questa scomposizione isola i vincoli della conversazione a turni, soprattutto la latenza, e permette di aggiornare i singoli moduli senza modificare l'intera architettura. Figura~\ref{fig:pipeline-ai-cap2} mostra il flusso dei dati dal segnale audio in ingresso al testo trascritto, fino al testo di risposta e all'audio sintetizzato.

\begin{figure}[t]
\centering
\includegraphics[width=1.00\textwidth]{pipeline_ai.png}
\caption{Schema della pipeline conversazionale (STT $\rightarrow$ RAG/LLM $\rightarrow$ TTS) adottata in SOULFRAME.}
\label{fig:pipeline-ai-cap2}
\end{figure}

\subsection{Speech-to-Text con Whisper}
In SOULFRAME, Whisper è scelto come base STT per la robustezza zero-shot in condizioni non controllate, evitando l'onere di addestrare un ASR dedicato al progetto. Il primo stadio della pipeline è il riconoscimento automatico del parlato: l'audio acquisito in push-to-talk viene trascritto e inoltrato ai moduli conversazionali successivi.

Whisper è un modello STT basato su architettura Transformer e addestrato su larga scala con supervisione debole. L'addestramento su circa 680.000 ore e l'impostazione multilingue (99 lingue) supportano buone prestazioni zero-shot e robustezza a variabilità di parlanti e condizioni acustiche.\cite{Radford2023Whisper} In questa sede interessa soprattutto il ruolo teorico del modello nella pipeline; configurazioni operative e profili di deploy sono discussi nei capitoli architetturali e implementativi.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{whisper_arch.png}
\caption{Architettura del modello Whisper: encoder Transformer con
log-mel spectrogram in input e decoder per la trascrizione
multilingue zero-shot.\protect\footnotemark}
\label{fig:whisper-arch-cap2}
\end{figure}
\footnotetext{Fonte: Radford et al., \emph{Robust Speech Recognition via
Large-Scale Weak Supervision}, ICML 2023, Fig.~1,
\url{https://arxiv.org/abs/2212.04356}.}

\subsection{Memoria conversazionale con RAG (LLM + embeddings + retrieval)}
In SOULFRAME, il paradigma RAG è scelto per aggiornare la memoria conversazionale senza riaddestrare il modello generativo a ogni variazione dei contenuti. Il secondo stadio introduce quindi una memoria che combina generazione e recupero di contesto.

La query testuale viene trasformata in embedding, usata per recuperare dall'indice i passaggi pertinenti e reinserita nel contesto fornito al Large Language Model (LLM). Così la risposta può mantenere continuità tra turni e riutilizzare informazioni già emerse nella conversazione.

L'approccio RAG integra memoria parametrica del modello e memoria non parametrica aggiornata via indice; la conoscenza può quindi evolvere intervenendo sui contenuti recuperabili senza riaddestrare i pesi.\cite{Lewis2020RAG} Figura~\ref{fig:rag-frontiers} mostra il flusso retrieval$\rightarrow$generation.

\begin{figure}[t]
\centering
\includegraphics[width=0.70\textwidth]{rag_frontiers_architecture.png}
\caption{Schema concettuale di Retrieval-Augmented Generation (RAG): recupero di contesto e generazione della risposta. \protect\footnotemark}
\label{fig:rag-frontiers}
\end{figure}
\footnotetext{Fonte: Kaur et al., \emph{Knowledge, context and personalization in retrieval-augmented generation for healthcare}, Frontiers in Artificial Intelligence (2025), \url{https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1697169/full} (Figura 1; licenza/uso: CC BY).}

Nel quadro della tesi, il punto rilevante è che RAG separa la conoscenza dinamica dal solo comportamento parametrico del modello, rendendo praticabile una memoria per-avatar che può essere estesa nel tempo. Le scelte operative su runtime, storage vettoriale e retrieval sono rinviate ai capitoli di architettura e implementazione.

\subsection{Text-to-Speech con Coqui XTTS v2}
In SOULFRAME, XTTS v2 è scelto per ottenere voce per-avatar in modalità zero-shot, evitando pipeline basate su risposte preregistrate statiche. Il terzo stadio converte il testo in audio e chiude il ciclo conversazionale: in un ECA embodied la sintesi influisce direttamente sulla percezione di continuità e coerenza dell'interlocutore.

Coqui XTTS v2 offre supporto multilingue e voice cloning tramite campione di riferimento, senza richiedere l'addestramento di un modello vocale dedicato per ogni utente.\cite{Casanova2024XTTS} In questo capitolo il focus resta sulla rilevanza teorica del modello per il problema di ricerca; i dettagli di configurazione e integrazione sono trattati nei capitoli successivi. Un riferimento architetturale del modello è riportato in Figura~\ref{fig:xtts-arch-cap2}.

\begin{figure}[t]
\centering
\includegraphics[width=0.65\textwidth]{xtts_arch.png}
\caption{Panoramica dell'architettura XTTS: Conditioning Encoder,
encoder GPT-2 con Perceiver Resampler e decoder HiFi-GAN per
sintesi multilingue zero-shot con voice cloning.\protect\footnotemark}
\label{fig:xtts-arch-cap2}
\end{figure}
\footnotetext{Fonte: Casanova et al., \emph{XTTS: a Massively Multilingual
Zero-Shot Text-to-Speech Model}, Interspeech 2024, Fig.~1,
\url{https://arxiv.org/abs/2406.04904}.}

\section{Posizionamento di SOULFRAME rispetto allo stato dell'arte}
La scelta di una pipeline locale a tre stadi nasce soprattutto dal problema della latenza percepita: ridurre dipendenze cloud aiuta a mantenere più prevedibile il ciclo audio$\rightarrow$testo$\rightarrow$risposta$\rightarrow$audio. In SOULFRAME, la continuità è sostenuta da una memoria persistente e isolata per avatar, che conserva contesto e preferenze senza interferenze tra profili distinti. Nel retrieval, la combinazione ibrida di segnali semantici e lessicali rende la risposta più robusta anche con query formulate in modo variabile. Per contenere la latenza percepita, il sistema usa sintesi in streaming e strategie di attesa che riducono i silenzi udibili. La modularità architetturale permette di sostituire singoli moduli senza modificare il client, favorendo iterazione tecnica e confrontabilità dei risultati.

Il posizionamento non riguarda solo la pipeline: in SOULFRAME l'avatar non è un semplice asset grafico, ma un profilo che combina aspetto, voce e memoria, così da mantenere coerenza tra sessioni e rendere più plausibile la presenza dell'interlocutore. Ho preferito vincolare l'accesso alla conversazione operativa alla disponibilità di un profilo minimo (voce e memoria), per evitare un'esperienza \textit{stateless} in cui ogni turno riparte da zero. La memoria non è limitata a note manuali: ingestione di documenti e immagini trasformano materiali esterni in contesto riutilizzabile e ampliano il perimetro oltre il dialogo puro. L'esecuzione via browser riduce la barriera di ingresso rispetto a configurazioni VR tradizionali, ma impone vincoli tecnici su microfono, gestione audio e latenza. In questo quadro diventano centrali streaming, warmup e segnali di attesa. Le implicazioni architetturali di queste decisioni, insieme ai componenti che le realizzano, vengono discusse nel Capitolo~\ref{chap:architettura}, mentre il Capitolo~\ref{chap:implementazione} entra nei dettagli implementativi e nei compromessi emersi durante lo sviluppo.
