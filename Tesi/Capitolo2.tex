\chapter{Fondamenti e Stato dell'Arte}
\label{chap:stato-arte}

\section{Agenti conversazionali embodied in XR}
\label{sec:eca-xr}
Gli Embodied Conversational Agents (ECA) sono agenti conversazionali dotati di una rappresentazione corporea, progettati per essere percepiti come interlocutori nello spazio di interazione. In Extended Reality (XR), il corpo virtuale viene collocato in una scena tridimensionale e coordinato con i turni del dialogo, con l'obiettivo di rendere l'interazione più naturale rispetto a un'interfaccia solo testuale.

La letteratura recente mostra che molti sistemi ECA in XR sono sviluppati in Virtual Reality (VR), spesso con Head-Mounted Display (HMD) e con Unity come piattaforma ricorrente. Risulta frequente anche l'uso dell'interazione vocale e di configurazioni uno-a-uno. Molte applicazioni restano orientate a compiti specifici, ma cresce l'interesse verso dialoghi più adattivi supportati da modelli neurali.\cite{Yang2025ECA_XR} SOULFRAME si colloca in questo scenario come ECA vocale embodied in un ambiente 3D fruibile via WebGL senza HMD: l'agente conversa in modalità push-to-talk e mantiene un profilo avatar che integra aspetto e voce.

\subsection{Presenza sociale, co-presenza e ruolo della voce}
Quando l'interazione avviene in un ambiente mediato, la presenza sociale descrive la percezione dell'altro come interlocutore con cui si condivide un'esperienza. Una trattazione utile per la valutazione scompone questo fenomeno in componenti distinguibili, includendo la co-presenza (percezione di condividere lo stesso spazio) e forme di coinvolgimento attentivo e comportamentale che rendono il dialogo più contingente e reciproco.\cite{Oh2018SocialPresence}

La voce è un canale rilevante per la presenza sociale perché rende immediatamente percepibili tempi di risposta, ritmo e prosodia. In VR questo effetto risulta più marcato quando l'agente è embodied: evidenze sperimentali indicano che un ECA con interazione vocale in tempo reale (STT+TTS) aumenta la co-presenza percepita rispetto a una condizione con audio pre-registrato.\cite{Kan2023ECA_Training} In SOULFRAME, la scelta del push-to-talk e l'uso di una voce persistente per avatar seguono questa logica, con l'obiettivo di stabilizzare i turni e mantenere coerenza d'identità.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{social_presence_audio_quality.png}
\caption{Fattori immersivi associati alla presenza sociale: la qualità audio compare tra le variabili considerate nella letteratura. \protect\footnotemark}
\label{fig:social-presence-audio}
\end{figure}
\footnotetext{Fonte: Oh et al., \emph{A Systematic Review of Social Presence: Definition, Antecedents, and Implications}, Frontiers in Robotics and AI (2018), \url{https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00114/full} (Figura 3; licenza/uso: CC BY).}

La presenza sociale e la co-presenza dipendono dall'allineamento tra canali comunicativi e tempi dell'interazione. Figura~\ref{fig:social-presence-audio} mostra che la qualità audio è una delle variabili considerate insieme ad altri fattori immersivi. Per un ECA vocale, qualità della voce e gestione temporale dei turni influenzano la naturalezza percepita; in SOULFRAME, push-to-talk, profilo vocale per avatar, warmup e frasi di attesa sono adottati per mitigare sovrapposizioni e silenzi durante l'elaborazione.

\subsection{Limiti aperti nei sistemi conversazionali immersivi}
L'integrazione del dialogo in ambienti 3D introduce criticità che emergono meno nei sistemi testuali tradizionali. La principale è la latenza end-to-end tra acquisizione audio, trascrizione, generazione e sintesi: ritardi percepibili riducono la naturalezza percepita anche quando il contenuto della risposta è corretto.

La continuità tra turni è un secondo limite: senza una memoria affidabile il sistema fatica a mantenere riferimenti e preferenze espresse dall'utente. Anche l'integrazione multimodale resta critica, perché richiede coerenza tra risposta linguistica, tempi e segnali non verbali. La valutazione, infine, è complessa: combina metriche soggettive (presenza, co-presenza, naturalezza) e metriche tecniche (latenza, errori di trascrizione), con protocolli non sempre uniformi.

In letteratura, architetture modulari open-source per ECA vocali in VR mostrano che la separazione in servizi STT e TTS semplifica l'integrazione ma rende evidenti i colli di bottiglia temporali, richiedendo strategie come output in streaming.\cite{Yin2023VoiceVR} SOULFRAME adotta una pipeline a tre stadi, memoria persistente per avatar e frasi di attesa per ridurre l'impatto dei tempi morti senza dipendenze cloud.

\section{Pipeline AI adottata in SOULFRAME}
\label{sec:pipeline-ai}
SOULFRAME implementa una pipeline conversazionale in tre stadi: riconoscimento del parlato, generazione contestuale e sintesi vocale. La scomposizione in tre stadi isola i vincoli della conversazione a turni, in particolare la latenza, e permette di aggiornare i singoli moduli senza modificare l'intera architettura. Figura~\ref{fig:pipeline-ai-cap2} mostra il flusso dei dati dal segnale audio in ingresso al testo trascritto, fino al testo di risposta e all'audio sintetizzato.

\begin{figure}[t]
\centering
\includegraphics[width=1.00\textwidth]{pipeline_ai.png}
\caption{Schema della pipeline conversazionale (STT $\rightarrow$ RAG/LLM $\rightarrow$ TTS) adottata in SOULFRAME.}
\label{fig:pipeline-ai-cap2}
\end{figure}

\subsection{Speech-to-Text con Whisper}
Il primo stadio della pipeline è il riconoscimento automatico del parlato. Nel prototipo, l'audio viene acquisito in push-to-talk dal client WebGL e inviato a un micro-servizio dedicato che restituisce la trascrizione da inoltrare al modulo di memoria e generazione. L'esecuzione locale riduce dipendenze di rete e supporta requisiti di privacy.

SOULFRAME utilizza Whisper, modello STT basato su architettura Transformer e addestrato su larga scala con supervisione debole. L'addestramento su circa 680.000 ore e l'impostazione multilingue (99 lingue) supportano buone prestazioni zero-shot e robustezza a variabilità di parlanti e condizioni acustiche.\cite{Radford2023Whisper} Nel prototipo è adottata la versione \texttt{medium} come compromesso tra qualità della trascrizione e costo computazionale. La struttura del modello è richiamata in Figura~\ref{fig:whisper-arch-cap2}.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{whisper_arch.png}
\caption{Architettura del modello Whisper: encoder Transformer con
log-mel spectrogram in input e decoder per la trascrizione
multilingue zero-shot.\protect\footnotemark}
\label{fig:whisper-arch-cap2}
\end{figure}
\footnotetext{Fonte: Radford et al., \emph{Robust Speech Recognition via
Large-Scale Weak Supervision}, ICML 2023, Fig.~1,
\url{https://arxiv.org/abs/2212.04356}.}

\subsection{Memoria conversazionale con RAG (LLM + embeddings + retrieval)}
Il secondo stadio introduce una memoria conversazionale che combina generazione e recupero di contesto.

In termini operativi, la query testuale viene trasformata in embedding, usata per recuperare dall'indice i passaggi pertinenti e reinserita nel contesto fornito al Large Language Model (LLM). In questo modo la risposta può mantenere continuità tra turni e riutilizzare informazioni già emerse nella conversazione.

L'approccio RAG integra memoria parametrica del modello e memoria non parametrica aggiornata via indice; la conoscenza può quindi evolvere intervenendo sui contenuti recuperabili senza riaddestrare i pesi.\cite{Lewis2020RAG} Figura~\ref{fig:rag-frontiers} mostra il flusso retrieval$\rightarrow$generation.

\begin{figure}[t]
\centering
\includegraphics[width=0.70\textwidth]{rag_frontiers_architecture.png}
\caption{Schema concettuale di Retrieval-Augmented Generation (RAG): recupero di contesto e generazione della risposta. \protect\footnotemark}
\label{fig:rag-frontiers}
\end{figure}
\footnotetext{Fonte: Kaur et al., \emph{Knowledge, context and personalization in retrieval-augmented generation for healthcare}, Frontiers in Artificial Intelligence (2025), \url{https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1697169/full} (Figura 1; licenza/uso: CC BY).}

Nel prototipo SOULFRAME la memoria è persistente tra sessioni e separata per avatar, per ridurre interferenze tra profili. Il servizio RAG integra LLM via Ollama, modello di embedding e ChromaDB; supporta note testuali e ingestione di documenti (con OCR per PDF) e adotta retrieval ibrido, combinando similarità semantica e segnali lessicali.

\subsection{Text-to-Speech con Coqui XTTS v2}
Il terzo stadio converte il testo in audio e chiude il ciclo conversazionale. In un ECA embodied, la sintesi vocale influisce sulla percezione di continuità e coerenza dell'interlocutore.

SOULFRAME adotta Coqui XTTS v2 per supporto multilingue e voice cloning tramite campione di riferimento, senza addestrare un modello vocale dedicato per ogni utente.\cite{Casanova2024XTTS} Nel prototipo, il campione è validato rispetto al testo atteso e associato al profilo avatar; il servizio supporta sintesi in streaming, warmup e frasi di attesa per mitigare la latenza percepita. Un riferimento architetturale del modello è riportato in Figura~\ref{fig:xtts-arch-cap2}.

\begin{figure}[t]
\centering
\includegraphics[width=0.65\textwidth]{xtts_arch.png}
\caption{Panoramica dell'architettura XTTS: Conditioning Encoder,
encoder GPT-2 con Perceiver Resampler e decoder HiFi-GAN per
sintesi multilingue zero-shot con voice cloning.\protect\footnotemark}
\label{fig:xtts-arch-cap2}
\end{figure}
\footnotetext{Fonte: Casanova et al., \emph{XTTS: a Massively Multilingual
Zero-Shot Text-to-Speech Model}, Interspeech 2024, Fig.~1,
\url{https://arxiv.org/abs/2406.04904}.}

\section{Posizionamento di SOULFRAME rispetto allo stato dell'arte}
I limiti aperti richiamati nella Sezione~\ref{sec:eca-xr} riguardano soprattutto latenza end-to-end, continuità tra turni, integrazione multimodale e valutazione sperimentale. In SOULFRAME ho scelto una pipeline locale a tre stadi per ridurre dipendenze cloud e mantenere più prevedibile il ciclo audio$\rightarrow$testo$\rightarrow$risposta$\rightarrow$audio. La continuità è sostenuta da una memoria persistente e isolata per avatar, che conserva contesto e preferenze senza interferenze tra profili distinti. Sul piano del retrieval, la combinazione ibrida di segnali semantici e lessicali rende più robusta la risposta anche con query formulate in modo variabile. Per contenere la latenza percepita durante il turno conversazionale, il sistema usa sintesi in streaming e strategie di attesa che riducono i silenzi udibili. La modularità architetturale consente di sostituire singoli moduli senza modificare il client, favorendo iterazione tecnica e confrontabilità dei risultati.

Il posizionamento non riguarda solo la pipeline: in SOULFRAME l'avatar non è un semplice asset grafico, ma un profilo che combina aspetto, voce e memoria, così da mantenere coerenza tra sessioni e rendere più plausibile la presenza dell'interlocutore. Ho preferito vincolare l'accesso alla conversazione operativa alla disponibilità di un profilo minimo (voce e memoria) per evitare un'esperienza \textit{stateless} in cui ogni turno riparte da zero e l'utente non percepisce progressione. La memoria non è limitata a note manuali: l'ingestione di documenti e immagini consente di trasformare materiali esterni in contesto riutilizzabile, ampliando il perimetro oltre il dialogo puro e rendendo più naturale riferirsi a contenuti che l'utente ha esplicitamente fornito. Sul piano dell'accessibilità, l'esecuzione via browser riduce la barriera di ingresso rispetto a configurazioni VR tradizionali, ma impone vincoli tecnici---acquisizione microfono, gestione dell'audio e latenza---che rendono centrali scelte come streaming, warmup e segnali di attesa. Le implicazioni architetturali di queste decisioni, insieme ai componenti che le realizzano, vengono discusse nel Capitolo~\ref{chap:architettura}, mentre il Capitolo~\ref{chap:implementazione} entra nei dettagli implementativi e nei compromessi emersi durante lo sviluppo.
